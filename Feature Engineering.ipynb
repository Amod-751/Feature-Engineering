{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.What is a parameter?\n",
    "    ->A parameter in ML is an internal variable that a model learns from the training data.\n",
    "        These are the values that define how the model makes predictions.\n",
    "        The learning algorithm adjusts parameters to minimize error (loss function).\n",
    "\n",
    "        Examples of Parameters in ML\n",
    "\n",
    "        Linear Regression:-\n",
    "\n",
    "        y = wx + b\n",
    "\n",
    "        w (weight/slope) and b (bias/intercept) are parameters that the algorithm learns during training to find the best fit.\n",
    "\n",
    "        Logistic Regression:-\n",
    "\n",
    "        The coefficients (w) determine how strongly each feature influences the probability prediction and are considered parameters.\n",
    "\n",
    "        Neural Networks:-\n",
    "\n",
    "        Each connection weight and bias in the network acts as a parameter. Deep learning models can contain millions or even billions of parameters.\n",
    "\n",
    "        Decision Trees:-\n",
    "\n",
    "        The split points and thresholds at each node are treated as parameters.\n",
    "        \n",
    "        \n",
    "2.What is correlation?\n",
    "What does negative correlation mean?\n",
    "    ->In machine learning, correlation describes the statistical relationship between features (independent variables) or between a feature and the target variable.\n",
    "    It measures how strongly two variables move together.\n",
    "    A correlation coefficient (usually Pearsonâ€™s r) tells us the strength (magnitude) and direction (positive or negative) of that relationship.\n",
    "\n",
    "    Why it matters in ML:\n",
    "\n",
    "        Feature selection: Highly correlated features may be redundant (multicollinearity).\n",
    "\n",
    "        Understanding data: Correlation between features and the target helps identify useful predictors.\n",
    "\n",
    "        Interpretability: Explains why a model might weigh features in a certain way.\n",
    "\n",
    "     What does Negative Correlation Mean in ML?\n",
    "\n",
    "        A negative correlation means that as one variable increases, the other tends to decrease.\n",
    "\n",
    "        In ML, if a feature has a negative correlation with the target, it means higher values of that feature are associated with lower target values (and vice versa).\n",
    "\n",
    "    Examples in ML:\n",
    "\n",
    "        =>House Prices Dataset\n",
    "\n",
    "            Feature: Distance from city center\n",
    "\n",
    "            Target: House price\n",
    "\n",
    "            Relationship: Negative correlation (farther from the city â†’ lower price).\n",
    "\n",
    "        =>Employee Attrition Prediction\n",
    "\n",
    "            Feature: Job satisfaction\n",
    "\n",
    "            Target: Probability of quitting\n",
    "\n",
    "            Relationship: Negative correlation (higher satisfaction â†’ lower chance of quitting).\n",
    "\n",
    "        =>Medical Dataset\n",
    "\n",
    "            Feature: Exercise per week\n",
    "\n",
    "            Target: Risk of heart disease\n",
    "\n",
    "            Relationship: Negative correlation (more exercise â†’ lower risk).\n",
    "            \n",
    "            \n",
    "3.Define Machine Learning. What are the main components in Machine Learning?\n",
    "    ->Machine Learning is a branch of Artificial Intelligence (AI) that focuses on creating algorithms and models that can learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
    "\n",
    "        In simple words:\n",
    "          Instead of writing rules manually, we feed data to a machine, and it learns rules/patterns automatically.\n",
    "\n",
    "          Main Components in Machine Learning\n",
    "\n",
    "        A typical ML system has 4 main components:\n",
    "\n",
    "        1. Data\n",
    "\n",
    "        The foundation of ML â€” raw information used for training and testing models.\n",
    "\n",
    "        Types:\n",
    "\n",
    "        Training data â†’ used to teach the model.\n",
    "\n",
    "        Validation data â†’ used to tune hyperparameters.\n",
    "\n",
    "        Test data â†’ used to evaluate performance.\n",
    "\n",
    "          Example: Customer purchase history, images, medical records.\n",
    "\n",
    "        2. Model\n",
    "\n",
    "        The mathematical structure/algorithm that makes predictions or decisions.\n",
    "\n",
    "        It defines the relationship between input features and output target.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        Linear Regression model: predicts house prices.\n",
    "\n",
    "        Neural Network: recognizes handwritten digits.\n",
    "\n",
    "        3. Learning Algorithm\n",
    "\n",
    "        The method/process used to adjust the modelâ€™s parameters (weights, biases) based on data.\n",
    "\n",
    "        Goal: Minimize error (loss function) and improve performance.\n",
    "\n",
    "          Examples:\n",
    "\n",
    "        Gradient Descent (optimizes weights).\n",
    "\n",
    "        Backpropagation (used in neural networks).\n",
    "\n",
    "        4. Prediction / Inference\n",
    "\n",
    "        Once trained, the model is used to predict outputs for new, unseen data.\n",
    "\n",
    "        This is the real-world application of the ML system.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        Predicting whether an email is spam.\n",
    "\n",
    "        Recommending movies on Netflix.\n",
    "\n",
    "        (Optional but Important) Supporting Components:\n",
    "\n",
    "        Features â†’ The input variables (e.g., age, income, image pixels).\n",
    "\n",
    "        Loss function â†’ Measures error between predictions and actual values.\n",
    "\n",
    "        Evaluation metrics â†’ Accuracy, precision, recall, RMSE, etc.\n",
    "\n",
    "4.How does loss value help in determining whether the model is good or not?\n",
    "    ->The loss function, also known as the cost function, measures the difference between the model's predictions and the actual values (ground truth). \n",
    "      It indicates how poorly the model is performingâ€”smaller loss means better predictions.\n",
    "\n",
    "        For example, in regression, Mean Squared Error (MSE) is commonly used:  \n",
    "        Loss = (1/n) Î£(y_true - y_pred)Â².  \n",
    "        If the predictions are close to the actual values, the loss will be small.\n",
    "\n",
    "        Loss plays a crucial role in determining model quality:  \n",
    "\n",
    "        - Training Guidance: The optimizer, like gradient descent, uses the loss value to adjust the model's parameters (weights and biases). A lower loss shows the model is learning meaningful patterns.  \n",
    "        - Model Comparison: Loss values can compare different models or hyperparameters. A consistently lower loss on validation or test data indicates a better model.  \n",
    "        - Overfitting/Underfitting Detection:  \n",
    "          - Training loss â†“, Validation loss â†“ â†’ Good fit.  \n",
    "          - Training loss â†“, Validation loss â†‘ â†’ Overfitting (memorizing training data).  \n",
    "          - High training and validation loss â†’ Underfitting (model too simple).  \n",
    "        - Early Stopping & Monitoring: In deep learning, loss curves help determine when to stop training to avoid overfitting.\n",
    "\n",
    "\n",
    "5.What are continuous and categorical variables?\n",
    "    ->1. Continuous Variables\n",
    "\n",
    "        A continuous variable can take any value within a range (including decimals/fractions).\n",
    "\n",
    "        They are measured, not counted.\n",
    "\n",
    "        Infinite possible values between two points.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "            Height (e.g., 172.3 cm)\n",
    "\n",
    "            Weight (e.g., 65.8 kg)\n",
    "\n",
    "            Temperature (e.g., 36.6 Â°C)\n",
    "\n",
    "            Time taken to finish a race (e.g., 12.54 seconds)\n",
    "\n",
    "     In ML: Continuous variables are often treated as numerical features and used in regression tasks.\n",
    "\n",
    "     2. Categorical Variables:-\n",
    "\n",
    "        A categorical variable represents distinct groups or categories.\n",
    "\n",
    "        They are counted, not measured.\n",
    "\n",
    "        Values are qualitative, not numerical (though sometimes encoded as numbers).\n",
    "\n",
    "        Types of Categorical Variables:\n",
    "\n",
    "        Nominal (no order):\n",
    "\n",
    "            Example: Colors (red, blue, green), Gender (male, female).\n",
    "\n",
    "        Ordinal (with order/ranking):\n",
    "\n",
    "            Example: Education level (High school < Bachelor < Master < PhD).\n",
    "\n",
    "            Example: Customer satisfaction (Poor < Fair < Good < Excellent).\n",
    "\n",
    "        In ML: Categorical variables need encoding (e.g., one-hot encoding, label encoding) before being fed into models.\n",
    "\n",
    "\n",
    "6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "    ->Models like Linear Regression, Logistic Regression, SVM, Neural Networks, etc., require numbers.\n",
    "\n",
    "        Categorical features (like color = red/blue/green) must be encoded into numeric form without losing information.\n",
    "\n",
    "        Common Techniques to Handle Categorical Variables\n",
    "        1. Label Encoding\n",
    "\n",
    "        Converts each category into an integer.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        Color: Red â†’ 0, Blue â†’ 1, Green â†’ 2\n",
    "\n",
    "\n",
    "          Pros: Simple, memory-efficient.\n",
    "          Cons: Implies ordinal relationship (0 < 1 < 2), which may mislead models.\n",
    "\n",
    "        2. One-Hot Encoding\n",
    "\n",
    "        Creates binary columns (0/1) for each category.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        Color: Red â†’ [1,0,0], Blue â†’ [0,1,0], Green â†’ [0,0,1]\n",
    "\n",
    "\n",
    "          Pros: No false ordering, widely used.\n",
    "          Cons: Can cause high dimensionality if many categories.\n",
    "\n",
    "        3. Ordinal Encoding (for ordered categories)\n",
    "\n",
    "        Assigns integers based on order/rank.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        Size: Small â†’ 1, Medium â†’ 2, Large â†’ 3\n",
    "\n",
    "\n",
    "          Pros: Preserves natural order.\n",
    "          Cons: Not suitable if categories have no ranking.\n",
    "\n",
    "        4. Frequency / Count Encoding\n",
    "\n",
    "        Replace each category with its frequency/count in the dataset.\n",
    "\n",
    "          Example:\n",
    "\n",
    "        City: Mumbai (100), Delhi (80), Bangalore (60)\n",
    "\n",
    "\n",
    "          Pros: Keeps some information about distribution.\n",
    "          Cons: Can still mislead models into thinking higher frequency means more importance.\n",
    "\n",
    "        5. Target Encoding (Mean Encoding)\n",
    "\n",
    "        Replace each category with the mean of target variable for that category.\n",
    "\n",
    "           Example: Predicting loan approval â†’ replace â€œJob Titleâ€ with average approval rate for that job.\n",
    "           Pros: Useful for high-cardinality features.\n",
    "           Cons: Risk of data leakage (must use CV).\n",
    "\n",
    "        6. Embedding Representations (Deep Learning)\n",
    "\n",
    "        Learn a dense vector representation for categories (used in neural networks).\n",
    "\n",
    "        Common in NLP (word embeddings like Word2Vec, GloVe).\n",
    "        \n",
    "        \n",
    "7.What do you mean by training and testing a dataset?\n",
    "    ->1. Training Dataset\n",
    "\n",
    "        The portion of data used to teach the model.\n",
    "\n",
    "        The model looks at the input features (X) and corresponding output labels (y) and learns patterns.\n",
    "\n",
    "        During training, the algorithm adjusts its parameters (like weights, biases) to minimize the loss function.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            Data: House size, number of rooms â†’ House price\n",
    "\n",
    "            Training: Model learns the relationship between features (size, rooms) and target (price).\n",
    "\n",
    "     2. Testing Dataset\n",
    "\n",
    "        The unseen portion of data used to evaluate the trained model.\n",
    "\n",
    "        It checks whether the model has actually learned general patterns (not just memorized training data).\n",
    "\n",
    "        Performance on the test set tells us if the model can generalize to new data.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            After training on 80% of housing data, we test on the remaining 20% to see how well the model predicts prices of houses it hasnâ€™t seen before.\n",
    "\n",
    "        Why Split into Training and Testing?\n",
    "\n",
    "            If we train and test on the same data, the model may perform well (low loss) but fail on new data â†’ this is called overfitting.\n",
    "\n",
    "            Splitting ensures we evaluate true predictive ability.\n",
    "\n",
    "        Typical Dataset Splits\n",
    "\n",
    "            Training set: 70â€“80%\n",
    "\n",
    "            Testing set: 20â€“30%\n",
    "\n",
    "            Sometimes, we also use a Validation set (10â€“20%) to tune hyperparameters.\n",
    "\n",
    "             Or we use cross-validation instead of a fixed split.\n",
    "\n",
    "        Quick Analogy\n",
    "\n",
    "            Training dataset = studying with practice questions.\n",
    "\n",
    "            Testing dataset = final exam with new questions.\n",
    "            \n",
    "            \n",
    "8.What is sklearn.preprocessing?\n",
    "    ->In scikit-learn (sklearn),\n",
    "        sklearn.preprocessing is a module that provides functions and classes for scaling, transforming, and encoding data before feeding it into a machine learning model.\n",
    "\n",
    "        In ML, raw data often isnâ€™t ready for models â€” we need to:\n",
    "\n",
    "            Normalize or standardize numerical features.\n",
    "\n",
    "            Encode categorical variables.\n",
    "\n",
    "            Generate polynomial features.\n",
    "\n",
    "            Handle missing values (via imputers, though found in sklearn.impute).\n",
    "\n",
    "            Thatâ€™s where sklearn.preprocessing comes in.\n",
    "\n",
    "        Common Tasks in sklearn.preprocessing\n",
    "            1. Feature Scaling\n",
    "\n",
    "                Most ML algorithms perform better if features are on a similar scale.\n",
    "\n",
    "                StandardScaler â†’ scales data to mean = 0, std = 1.\n",
    "\n",
    "                MinMaxScaler â†’ scales data to a fixed range (usually [0, 1]).\n",
    "\n",
    "                RobustScaler â†’ uses median & IQR (robust to outliers).\n",
    "\n",
    "            2. Encoding Categorical Features\n",
    "\n",
    "                LabelEncoder â†’ converts categories to integer labels.\n",
    "\n",
    "                OneHotEncoder â†’ creates binary columns for each category.\n",
    "\n",
    "                OrdinalEncoder â†’ encodes categories with an order (e.g., small < medium < large).\n",
    "\n",
    "            3. Feature Transformation\n",
    "\n",
    "                PolynomialFeatures â†’ generates polynomial & interaction terms.\n",
    "\n",
    "                Binarizer â†’ converts numerical values to 0/1 based on a threshold.\n",
    "\n",
    "                Normalizer â†’ scales rows to have unit norm (for text mining, cosine similarity, etc.).\n",
    "\n",
    "            4. Custom Transformation Pipelines\n",
    "\n",
    "                FunctionTransformer â†’ apply custom transformation functions.\n",
    "\n",
    "                Works seamlessly with Pipelines (sklearn.pipeline.Pipeline) to chain preprocessing + model training steps.\n",
    "\n",
    "\n",
    "9.What is a Test set?\n",
    "    ->In Machine Learning, a test set is a portion of the dataset that is kept separate from the training data and used to evaluate the performance of a trained model on unseen data.\n",
    "\n",
    "        The model never sees this data during training.\n",
    "\n",
    "        It acts as a proxy for how the model will perform in the real world.\n",
    "\n",
    "        Purpose of a Test Set\n",
    "\n",
    "            Evaluate Generalization\n",
    "\n",
    "            Checks if the model can predict accurately on new, unseen data.\n",
    "\n",
    "            Detect Overfitting\n",
    "\n",
    "            If the model performs well on training data but poorly on the test set â†’ it is overfitting.\n",
    "\n",
    "            Compare Models\n",
    "\n",
    "            Helps in choosing the best model or algorithm based on its performance on unseen data.\n",
    "\n",
    "        Typical Split\n",
    "\n",
    "            Common practice:\n",
    "\n",
    "                Training set: 70â€“80% (to train the model)\n",
    "\n",
    "                Test set: 20â€“30% (to evaluate the model)\n",
    "\n",
    "            Sometimes a validation set is also used (or cross-validation) to tune hyperparameters before final testing.\n",
    "\n",
    "        Analogy\n",
    "\n",
    "            Training set: practice questions you study to learn.\n",
    "\n",
    "            Test set: final exam questions youâ€™ve never seen before.\n",
    "\n",
    "        Key Points\n",
    "\n",
    "            Test set must be representative of the real-world data.\n",
    "\n",
    "            Performance metrics on the test set (accuracy, RMSE, F1-score, etc.) are used to judge the model.\n",
    "            \n",
    "            \n",
    "10.How do we split data for model fitting (training and testing) in Python?\n",
    "How do you approach a Machine Learning problem?\n",
    "    ->In Machine Learning, the dataset is usually divided into training and testing sets to evaluate a modelâ€™s performance and its ability to generalize to unseen data.\n",
    "\n",
    "        Training Set:\n",
    "\n",
    "            A subset of the data used to train the model.\n",
    "\n",
    "            The model learns patterns, relationships, and parameters (like weights in regression or neural networks) from this data.\n",
    "\n",
    "        Test Set:\n",
    "\n",
    "            A separate subset of the data not seen by the model during training.\n",
    "\n",
    "            Used to assess the modelâ€™s performance on new, unseen data and check its generalization ability.\n",
    "\n",
    "        Typical Split Ratios:\n",
    "\n",
    "            Training: 70â€“80% of the data\n",
    "\n",
    "            Testing: 20â€“30% of the data\n",
    "\n",
    "        Notes:\n",
    "\n",
    "            Sometimes, a validation set is also used to fine-tune hyperparameters.\n",
    "\n",
    "            Splitting ensures the model does not overfit to the training data.\n",
    "\n",
    "     How to Approach a Machine Learning Problem\n",
    "\n",
    "        A structured approach is necessary for solving ML problems effectively. The common steps are:\n",
    "\n",
    "        Step 1: Problem Definition\n",
    "\n",
    "            Understand the objective of the task: classification, regression, clustering, etc.\n",
    "\n",
    "            Identify input features (independent variables) and the output/target (dependent variable).\n",
    "\n",
    "        Step 2: Data Collection\n",
    "\n",
    "            Gather data from relevant sources such as databases, files, sensors, or APIs.\n",
    "\n",
    "            Ensure data quality, consistency, and sufficiency.\n",
    "\n",
    "        Step 3: Data Exploration and Preprocessing\n",
    "\n",
    "            Perform Exploratory Data Analysis (EDA) to understand data distribution, patterns, and relationships.\n",
    "\n",
    "            Handle missing values, outliers, and inconsistencies.\n",
    "\n",
    "            Encode categorical variables and scale/normalize numerical features as required.\n",
    "\n",
    "        Step 4: Data Splitting\n",
    "\n",
    "            Divide data into training and testing sets to evaluate the modelâ€™s performance.\n",
    "\n",
    "            Optionally, create a validation set for hyperparameter tuning.\n",
    "\n",
    "        Step 5: Model Selection\n",
    "\n",
    "            Choose an appropriate algorithm based on the problem type.\n",
    "\n",
    "            Regression â†’ Linear Regression, Decision Trees, etc.\n",
    "\n",
    "            Classification â†’ Logistic Regression, SVM, Random Forest, etc.\n",
    "\n",
    "            Clustering â†’ K-Means, DBSCAN, etc.\n",
    "\n",
    "        Step 6: Model Training\n",
    "\n",
    "            Fit the selected model on the training set so it can learn the relationships between features and target.\n",
    "\n",
    "        Step 7: Model Evaluation\n",
    "\n",
    "            Use the test set to evaluate model performance using metrics appropriate for the problem:\n",
    "\n",
    "            Regression â†’ MSE, RMSE, RÂ²\n",
    "\n",
    "            Classification â†’ Accuracy, Precision, Recall, F1-score\n",
    "\n",
    "        Step 8: Hyperparameter Tuning\n",
    "\n",
    "            Optimize the model using techniques like Grid Search, Random Search, or Cross-Validation to improve performance.\n",
    "\n",
    "        Step 9: Deployment and Monitoring\n",
    "\n",
    "            Deploy the trained model to make predictions on real-world data.\n",
    "\n",
    "            Monitor performance continuously and update the model as needed.\n",
    "\n",
    "\n",
    "11.Why do we have to perform EDA before fitting a model to the data?\n",
    "    ->Exploratory Data Analysis (EDA) is the process of analyzing and visualizing data to understand its structure, patterns, and relationships before building a machine learning model.\n",
    "\n",
    "        Performing EDA is important because it helps in several key ways:\n",
    "\n",
    "        => Understand Data Distribution\n",
    "\n",
    "            EDA helps us see how features are distributed (normal, skewed, uniform, etc.).\n",
    "\n",
    "            Knowing the distribution allows us to choose appropriate preprocessing steps (like scaling, normalization, or transformation).\n",
    "\n",
    "            Example:\n",
    "\n",
    "                A highly skewed feature might need log transformation before training a regression model.\n",
    "\n",
    "        => Detect and Handle Missing Values\n",
    "\n",
    "            Real-world datasets often have missing or null values.\n",
    "\n",
    "            EDA helps identify missing data and decide how to handle it:\n",
    "\n",
    "            Drop rows/columns\n",
    "\n",
    "            Impute with mean, median, mode, or predictive methods\n",
    "\n",
    "        => Identify Outliers\n",
    "\n",
    "            Outliers can distort model training and reduce performance.\n",
    "\n",
    "            EDA helps spot extreme values using visualizations like boxplots or scatter plots.\n",
    "\n",
    "        => Discover Relationships Between Variables\n",
    "\n",
    "            EDA helps understand correlations and interactions between features and target variable.\n",
    "\n",
    "            This can guide feature selection and reduce multicollinearity.\n",
    "\n",
    "            Example:\n",
    "\n",
    "                A feature highly correlated with the target is likely useful for prediction.\n",
    "\n",
    "                Features strongly correlated with each other may need dimensionality reduction.\n",
    "\n",
    "        => Detect Data Quality Issues\n",
    "\n",
    "            EDA reveals inconsistencies, duplicate records, or errors in data.\n",
    "\n",
    "            Fixing these ensures the model learns true patterns rather than noise.\n",
    "\n",
    "        => Guide Feature Engineering\n",
    "\n",
    "            EDA provides insights for creating new features or transforming existing features to improve model performance.\n",
    "\n",
    "        => Choose the Right Model\n",
    "\n",
    "            Based on the patterns observed during EDA, you can decide whether to use:\n",
    "\n",
    "            Linear vs non-linear models\n",
    "\n",
    "            Simple vs complex models\n",
    "\n",
    "            Regression vs classification\n",
    "\n",
    "\n",
    "12.What is correlation?\n",
    "    ->In machine learning, correlation describes the statistical relationship between features (independent variables) or between a feature and the target variable.\n",
    "    It measures how strongly two variables move together.\n",
    "    A correlation coefficient (usually Pearsonâ€™s r) tells us the strength (magnitude) and direction (positive or negative) of that relationship.\n",
    "\n",
    "    Why it matters in ML:\n",
    "\n",
    "        Feature selection: Highly correlated features may be redundant (multicollinearity).\n",
    "\n",
    "        Understanding data: Correlation between features and the target helps identify useful predictors.\n",
    "\n",
    "        Interpretability: Explains why a model might weigh features in a certain way.\n",
    "\n",
    "13. What does Negative Correlation Mean?\n",
    "    ->A negative correlation means that as one variable increases, the other tends to decrease.\n",
    "\n",
    "        In ML, if a feature has a negative correlation with the target, it means higher values of that feature are associated with lower target values (and vice versa).\n",
    "\n",
    "    Examples in ML:\n",
    "\n",
    "        =>House Prices Dataset\n",
    "\n",
    "            Feature: Distance from city center\n",
    "\n",
    "            Target: House price\n",
    "\n",
    "            Relationship: Negative correlation (farther from the city â†’ lower price).\n",
    "\n",
    "        =>Employee Attrition Prediction\n",
    "\n",
    "            Feature: Job satisfaction\n",
    "\n",
    "            Target: Probability of quitting\n",
    "\n",
    "            Relationship: Negative correlation (higher satisfaction â†’ lower chance of quitting).\n",
    "\n",
    "        =>Medical Dataset\n",
    "\n",
    "            Feature: Exercise per week\n",
    "\n",
    "            Target: Risk of heart disease\n",
    "\n",
    "            Relationship: Negative correlation (more exercise â†’ lower risk).\n",
    "                      \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e08ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Age    Salary  Experience\n",
      "Age         1.000000  0.995940    0.990271\n",
      "Salary      0.995940  1.000000    0.986608\n",
      "Experience  0.990271  0.986608    1.000000\n"
     ]
    }
   ],
   "source": [
    "#14.How can you find correlation between variables in Python?\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset\n",
    "data = pd.DataFrame({\n",
    "    \"Age\": [25, 32, 47, 51, 23, 40],\n",
    "    \"Salary\": [50000, 60000, 80000, 90000, 45000, 70000],\n",
    "    \"Experience\": [2, 5, 20, 25, 1, 15]\n",
    "})\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "15.What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    "    ->Causation (also called a causal relationship) occurs when a change in one variable directly leads to a change in another variable.\n",
    "\n",
    "        It establishes a cause-and-effect relationship.\n",
    "\n",
    "        Causation implies that manipulating one variable will produce a predictable change in the other.\n",
    "\n",
    "        Determining causation usually requires controlled experiments, longitudinal studies, or strong statistical evidence.\n",
    "\n",
    "        Example of Causation:\n",
    "\n",
    "        Smoking â†’ Lung cancer\n",
    "\n",
    "        Scientific studies show that smoking cigarettes increases the risk of lung cancer.\n",
    "\n",
    "        Here, smoking is the cause, and lung cancer is the effect.\n",
    "\n",
    "        Exercise â†’ Weight loss\n",
    "\n",
    "        Regular exercise can lead to weight reduction.\n",
    "\n",
    "        Exercise is the causal factor, weight loss is the outcome.\n",
    "\n",
    "        Key Point: Causation implies a mechanism or rationale for why the change occurs.\n",
    "    \n",
    "    \n",
    "    =>Correlation measures the statistical relationship between two variables â€” how they move together.\n",
    "\n",
    "        It shows strength and direction (positive or negative).\n",
    "\n",
    "        Correlation does not imply causation â€” it only shows that two variables are related.\n",
    "\n",
    "        Example of Correlation:\n",
    "\n",
    "        Ice cream sales â†‘ and drowning incidents â†‘ in summer\n",
    "\n",
    "        These two variables are positively correlated.\n",
    "\n",
    "        However, ice cream sales do not cause drowning incidents.\n",
    "\n",
    "        The hidden factor is temperature/season â€” hot weather increases both swimming (leading to drownings) and ice cream consumption.\n",
    "\n",
    "        Key Point: Correlation is an association, not proof of cause-and-effect\n",
    "        \n",
    "        \n",
    "        | Aspect             | Correlation                                                                     | Causation                                                                   |\n",
    "| ------------------ | ------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| Definition     | Measures the strength and direction of a relationship between two variables | One variable directly influences or causes a change in another variable |\n",
    "| Nature         | Statistical association; can be positive, negative, or zero                     | Cause-and-effect relationship                                               |\n",
    "| Directionality | No causality implied; could be bidirectional or influenced by a third variable  | Always directional: cause â†’ effect                                          |\n",
    "| Proof Required | No proof needed; observed from data                                             | Requires experiments, longitudinal studies, or strong evidence              |\n",
    "| Example        | Ice cream sales and drowning deaths rise together in summer                     | Smoking causes lung cancer; exercise causes weight loss                     |\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "    ->In Machine Learning (especially in Neural Networks / Deep Learning), an optimizer is an algorithm used to update the parameters (weights and biases) of the model to minimize the loss function.\n",
    "\n",
    "        The goal of an optimizer is to find the best set of parameters that results in lowest error.\n",
    "\n",
    "        Optimizers guide the model during training by adjusting weights based on the gradient of the loss function.\n",
    "\n",
    "     Key Concept:\n",
    "\n",
    "        Most optimizers use gradient descent or its variants.\n",
    "\n",
    "        Gradient descent moves in the direction of steepest descent to minimize the loss.\n",
    "\n",
    "    Types of Optimizers\n",
    "\n",
    "        There are several types of optimizers. The common ones are:\n",
    "\n",
    "        A) Gradient Descent (GD)\n",
    "\n",
    "        The most basic optimizer.\n",
    "\n",
    "        Updates weights by moving in the negative direction of the gradient of the loss function.\n",
    "\n",
    "        Update Rule:\n",
    "        ð‘¤ = ð‘¤ âˆ’ ðœ‚ â‹… âˆ‡ð¿(ð‘¤)\n",
    "\n",
    "        Where:\n",
    "\n",
    "        ð‘¤ = weights  \n",
    "        ðœ‚ = learning rate  \n",
    "        âˆ‡ð¿(ð‘¤) = gradient of the loss function  \n",
    "\n",
    "    Variants of Gradient Descent:\n",
    "\n",
    "    Batch Gradient Descent:\n",
    "\n",
    "        Uses all training samples to compute the gradient.\n",
    "\n",
    "        Accurate but slow for large datasets.\n",
    "\n",
    "    Stochastic Gradient Descent (SGD):\n",
    "\n",
    "        Uses one training sample at a time to update weights.\n",
    "\n",
    "        Faster, introduces randomness â†’ can escape local minima.\n",
    "\n",
    "    Mini-Batch Gradient Descent:\n",
    "\n",
    "        Uses a subset (batch) of data to compute gradients.\n",
    "\n",
    "        Combines advantages of batch and stochastic GD.\n",
    "\n",
    "    B) Momentum Optimizer\n",
    "\n",
    "        Accelerates SGD by adding a momentum term, which helps smooth out oscillations in updates.\n",
    "\n",
    "        Updates are influenced by previous gradients.\n",
    "\n",
    "        Update Rule:\n",
    "            ð‘£ð‘¡ = ð›¾ð‘£ð‘¡âˆ’1 + ðœ‚âˆ‡ð¿(ð‘¤)  \n",
    "            ð‘¤ = ð‘¤ âˆ’ ð‘£ð‘¡  \n",
    "\n",
    "            Here, ð‘£ð‘¡ represents the velocity term, and ð›¾ is the momentum coefficient, typically set to 0.9.\n",
    "         Example:\n",
    "\n",
    "            Helps in scenarios where the loss surface has narrow valleys, like deep networks.\n",
    "\n",
    "    C) AdaGrad (Adaptive Gradient)\n",
    "\n",
    "        Adjusts the learning rate for each parameter individually based on past gradients.\n",
    "\n",
    "        Parameters with frequent updates get smaller learning rates.\n",
    "\n",
    "      Good For:\n",
    "\n",
    "            Sparse data (like text or NLP tasks)\n",
    "\n",
    "      Limitation:\n",
    "\n",
    "            Learning rate can shrink too much over time.\n",
    "\n",
    "    D) RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "        Improves on AdaGrad by using a decaying average of squared gradients to prevent learning rate from shrinking too much.\n",
    "\n",
    "        Popular in RNNs and deep learning tasks.\n",
    "\n",
    "    E) Adam (Adaptive Moment Estimation)\n",
    "\n",
    "        Combines Momentum + RMSProp.\n",
    "\n",
    "        Maintains moving averages of gradients and squared gradients.\n",
    "\n",
    "        Automatically adapts learning rate for each parameter.\n",
    "\n",
    "        Update Rule:\n",
    "\n",
    "            ð‘šâ‚œ = ð›½â‚ð‘šâ‚œâ‚‹â‚ + (1 âˆ’ ð›½â‚)âˆ‡ð¿(ð‘¤)  \n",
    "            ð‘£â‚œ = ð›½â‚‚ð‘£â‚œâ‚‹â‚ + (1 âˆ’ ð›½â‚‚)(âˆ‡ð¿(ð‘¤))Â²  \n",
    "            ð‘¤ = ð‘¤ âˆ’ ðœ‚ * ð‘šâ‚œ / (âˆšð‘£â‚œ + ðœ–)  \n",
    "\n",
    "\n",
    "\n",
    "        Pros:\n",
    "            Works well out-of-the-box for most neural networks.\n",
    "            Default optimizer in TensorFlow and PyTorch.\n",
    "\n",
    "    F) Other Optimizers\n",
    "\n",
    "        Adadelta â†’ Like RMSProp, but no need to set initial learning rate.\n",
    "\n",
    "        Nadam â†’ Adam + Nesterov momentum.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624b3850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "17.What is sklearn.linear_model ?\n",
    "    ->sklearn.linear_model is a module in scikit-learn (sklearn), a popular Python library for machine learning. This module provides classes and functions to implement linear models â€” models that assume a linear relationship between the input variables (features) and the output (target).\n",
    "\n",
    "        In simpler terms, itâ€™s used when you want to predict a value (or class) as a linear combination of features.\n",
    "\n",
    "        Key Features of sklearn.linear_model:\n",
    "\n",
    "        Regression Models â€“ Predict continuous values:\n",
    "\n",
    "            LinearRegression: Standard linear regression.\n",
    "\n",
    "            Ridge: Linear regression with L2 regularization (helps prevent overfitting).\n",
    "\n",
    "            Lasso: Linear regression with L1 regularization (can shrink some coefficients to zero).\n",
    "\n",
    "            ElasticNet: Combines L1 and L2 regularization.\n",
    "\n",
    "        Classification Models â€“ Predict categorical values:\n",
    "\n",
    "            LogisticRegression: For binary or multiclass classification.\n",
    "\n",
    "        Robust Models â€“ Resistant to outliers:\n",
    "\n",
    "            RANSACRegressor: Fits a model robustly by ignoring outliers.\n",
    "\n",
    "            HuberRegressor: Less sensitive to outliers in regression.\n",
    "\n",
    "        Other Linear Models:\n",
    "\n",
    "            SGDRegressor / SGDClassifier: Uses stochastic gradient descent for optimization.\n",
    "\n",
    "            Perceptron: Simple linear binary classifier.\n",
    "\n",
    "'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Create linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict([[6]])\n",
    "print(pred)  # Output: [12.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d91a66df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "18.What does model.fit() do? What arguments must be given?\n",
    "    ->In scikit-learn, model.fit() is the method used to train a machine learning model on your dataset. Essentially, it tells the model: â€œHereâ€™s the data and the target values â€” learn the patterns from this.â€\n",
    "\n",
    "        What model.fit() does:\n",
    "\n",
    "            Takes input features and target values (training data).\n",
    "\n",
    "            Calculates the model parameters (like weights in linear regression or coefficients in logistic regression) that best map inputs to outputs.\n",
    "\n",
    "            Stores the learned parameters inside the model object, so you can use model.predict() later.\n",
    "\n",
    "        Arguments for model.fit():\n",
    "\n",
    "            X â€“ Input features (independent variables)\n",
    "\n",
    "            Should be array-like of shape (n_samples, n_features).\n",
    "\n",
    "            Example: 2D array or pandas DataFrame where each row is a sample and each column is a feature.\n",
    "\n",
    "            y â€“ Target values (dependent variable)\n",
    "\n",
    "            Should be array-like of shape (n_samples,) for regression or (n_samples,) / (n_samples, n_outputs) for classification.\n",
    "\n",
    "            Example: 1D array or pandas Series for labels.\n",
    "\n",
    "        Optional arguments (depends on the model):\n",
    "\n",
    "        Some models accept extra arguments like sample_weight to give different importance to different samples.\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Features and target\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Model has learned the parameters\n",
    "print(model.coef_)   # [2.]\n",
    "print(model.intercept_)  # 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dcf759b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12. 14.]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "19.What does model.predict() do? What arguments must be given?\n",
    "    ->In scikit-learn, model.predict() is used to make predictions using a trained model. Essentially, after youâ€™ve trained your model with model.fit(), you can feed it new data and it will output predicted values or classes based on the patterns it learned.\n",
    "\n",
    "    What model.predict() does:\n",
    "\n",
    "        Takes new input data (features) that the model hasnâ€™t seen.\n",
    "\n",
    "        Uses the learned parameters from model.fit() to calculate predictions.\n",
    "\n",
    "        For regression, it predicts continuous values.\n",
    "\n",
    "            For classification, it predicts class labels.\n",
    "\n",
    "            Arguments for model.predict():\n",
    "\n",
    "    X â€“ Input features for which you want predictions.\n",
    "\n",
    "        Should be array-like of shape (n_samples, n_features).\n",
    "\n",
    "        Example: 2D array or pandas DataFrame where each row is a sample and each column is a feature.\n",
    "    \n",
    "    Important: The number of features (n_features) must match the training data used in .fit().\n",
    "'''\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Training data\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Create and train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# New data for prediction\n",
    "X_new = np.array([[6], [7]])\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict(X_new)\n",
    "print(predictions)  # Output: [12. 14.]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a624b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "20.What are continuous and categorical variables?\n",
    "    ->1. Continuous Variables\n",
    "\n",
    "        A continuous variable can take any value within a range (including decimals/fractions).\n",
    "\n",
    "        They are measured, not counted.\n",
    "\n",
    "        Infinite possible values between two points.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "            Height (e.g., 172.3 cm)\n",
    "\n",
    "            Weight (e.g., 65.8 kg)\n",
    "\n",
    "            Temperature (e.g., 36.6 Â°C)\n",
    "\n",
    "            Time taken to finish a race (e.g., 12.54 seconds)\n",
    "\n",
    "     In ML: Continuous variables are often treated as numerical features and used in regression tasks.\n",
    "\n",
    "     2. Categorical Variables:-\n",
    "\n",
    "        A categorical variable represents distinct groups or categories.\n",
    "\n",
    "        They are counted, not measured.\n",
    "\n",
    "        Values are qualitative, not numerical (though sometimes encoded as numbers).\n",
    "\n",
    "        Types of Categorical Variables:\n",
    "\n",
    "        Nominal (no order):\n",
    "\n",
    "            Example: Colors (red, blue, green), Gender (male, female).\n",
    "\n",
    "        Ordinal (with order/ranking):\n",
    "\n",
    "            Example: Education level (High school < Bachelor < Master < PhD).\n",
    "\n",
    "            Example: Customer satisfaction (Poor < Fair < Good < Excellent).\n",
    "\n",
    "        In ML: Categorical variables need encoding (e.g., one-hot encoding, label encoding) before being fed into models.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1783c021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "21.What is feature scaling? How does it help in Machine Learning?\n",
    "    ->Feature scaling is a data preprocessing technique in machine learning that normalizes or standardizes the range of independent variables (features). Essentially, it ensures that all features contribute equally to the model, especially when the features have very different scales.\n",
    "\n",
    "    Why Feature Scaling is Important:\n",
    "\n",
    "        Equal Contribution of Features\n",
    "\n",
    "            Many algorithms (like gradient descent-based models) calculate distances or weights.\n",
    "\n",
    "            If one feature ranges from 0â€“1 and another from 0â€“1000, the larger-scale feature will dominate, skewing the results.\n",
    "\n",
    "        Faster Convergence\n",
    "\n",
    "            Algorithms like gradient descent converge faster when features are on a similar scale.\n",
    "\n",
    "            Helps reduce training time.\n",
    "\n",
    "        Improved Accuracy in Distance-Based Models\n",
    "\n",
    "            Models like K-Nearest Neighbors (KNN), K-Means, and SVM rely on distance metrics.\n",
    "\n",
    "            Without scaling, features with larger ranges dominate the distance calculations.\n",
    "\n",
    "        Helps Regularization\n",
    "\n",
    "            For models like Ridge and Lasso regression, scaling ensures that regularization penalizes all features equally.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 100],\n",
    "              [2, 200],\n",
    "              [3, 300]])\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f34bb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.22474487 -1.22474487]\n",
      " [ 0.          0.        ]\n",
      " [ 1.22474487  1.22474487]]\n",
      "Centers the data around 0 with unit standard deviation.\n",
      "\n",
      "[[0.  0. ]\n",
      " [0.5 0.5]\n",
      " [1.  1. ]]\n",
      "Scales features to a fixed range [0,1].\n",
      "\n",
      "[[0.33333333 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [1.         1.        ]]\n",
      "Scales data by the maximum absolute value, keeps sign.\n",
      "\n",
      "[[-1. -1.]\n",
      " [ 0.  0.]\n",
      " [ 1.  1.]]\n",
      "Less sensitive to outliers; uses median and IQR.\n"
     ]
    }
   ],
   "source": [
    "#22.How do we perform scaling in Python?\n",
    "    #1. Standardization (Z-score Scaling)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 100],\n",
    "              [2, 200],\n",
    "              [3, 300]])\n",
    "\n",
    "# Create scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(X_scaled)\n",
    "print(\"Centers the data around 0 with unit standard deviation.\\n\")\n",
    "\n",
    "    \n",
    "    #2. Min-Max Scaling (Normalization)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "print(\"Scales features to a fixed range [0,1].\\n\")\n",
    "\n",
    "    \n",
    "    #3. Max Abs Scaling\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "print(\"Scales data by the maximum absolute value, keeps sign.\\n\")\n",
    "   \n",
    "    \n",
    "    #4. Robust Scaling\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "print(\"Less sensitive to outliers; uses median and IQR.\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "23.What is sklearn.preprocessing?\n",
    "    ->In scikit-learn (sklearn),\n",
    "        sklearn.preprocessing is a module that provides functions and classes for scaling, transforming, and encoding data before feeding it into a machine learning model.\n",
    "\n",
    "        In ML, raw data often isnâ€™t ready for models â€” we need to:\n",
    "\n",
    "            Normalize or standardize numerical features.\n",
    "\n",
    "            Encode categorical variables.\n",
    "\n",
    "            Generate polynomial features.\n",
    "\n",
    "            Handle missing values (via imputers, though found in sklearn.impute).\n",
    "\n",
    "            Thatâ€™s where sklearn.preprocessing comes in.\n",
    "\n",
    "        Common Tasks in sklearn.preprocessing\n",
    "            1. Feature Scaling\n",
    "\n",
    "                Most ML algorithms perform better if features are on a similar scale.\n",
    "\n",
    "                StandardScaler â†’ scales data to mean = 0, std = 1.\n",
    "\n",
    "                MinMaxScaler â†’ scales data to a fixed range (usually [0, 1]).\n",
    "\n",
    "                RobustScaler â†’ uses median & IQR (robust to outliers).\n",
    "\n",
    "            2. Encoding Categorical Features\n",
    "\n",
    "                LabelEncoder â†’ converts categories to integer labels.\n",
    "\n",
    "                OneHotEncoder â†’ creates binary columns for each category.\n",
    "\n",
    "                OrdinalEncoder â†’ encodes categories with an order (e.g., small < medium < large).\n",
    "\n",
    "            3. Feature Transformation\n",
    "\n",
    "                PolynomialFeatures â†’ generates polynomial & interaction terms.\n",
    "\n",
    "                Binarizer â†’ converts numerical values to 0/1 based on a threshold.\n",
    "\n",
    "                Normalizer â†’ scales rows to have unit norm (for text mining, cosine similarity, etc.).\n",
    "\n",
    "            4. Custom Transformation Pipelines\n",
    "\n",
    "                FunctionTransformer â†’ apply custom transformation functions.\n",
    "\n",
    "                Works seamlessly with Pipelines (sklearn.pipeline.Pipeline) to chain preprocessing + model training steps.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0251173c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[ 1]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 7]]\n",
      "X_test: [[9]\n",
      " [2]\n",
      " [6]]\n",
      "y_train: [ 1  8  3 10  5  4  7]\n",
      "y_test: [9 2 6]\n"
     ]
    }
   ],
   "source": [
    "#24.How do we split data for model fitting (training and testing) in Python?\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Split data: 70% training, 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_train)\n",
    "print(\"X_test:\", X_test)\n",
    "print(\"y_train:\", y_train)\n",
    "print(\"y_test:\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9787d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1]\n",
      "****************************\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]]\n",
      "****************************\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "25.Explain data encoding?\n",
    "  ->Data encoding is a preprocessing step in machine learning where categorical or non-numeric data is converted into a numeric format so that models can understand and use it. Most machine learning algorithms require numeric input, so encoding is essential for features like categories, labels, or text.\n",
    "\n",
    "    Why Data Encoding is Important:\n",
    "\n",
    "        ML algorithms need numbers\n",
    "\n",
    "            Algorithms like linear regression, logistic regression, SVM, and neural networks cannot directly process text or categories.\n",
    "\n",
    "        Preserve information\n",
    "\n",
    "            Encoding transforms categories into numbers while keeping the meaningful relationships between them (depending on the encoding type).\n",
    "\n",
    "        Avoid model bias\n",
    "\n",
    "            Encoding ensures that numeric representations donâ€™t mislead the model (e.g., 0, 1, 2 should not imply ranking unless intended).\n",
    "\n",
    "'''\n",
    "\n",
    "'''Common Types of Data Encoding:\n",
    "1. Label Encoding\n",
    "\n",
    "    Converts each category into a unique integer.\n",
    "\n",
    "    Good for ordinal data (where order matters, e.g., Low < Medium < High).\n",
    "'''\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "categories = ['Red', 'Green', 'Blue', 'Green']\n",
    "encoder = LabelEncoder()\n",
    "encoded = encoder.fit_transform(categories)\n",
    "print(encoded)  # Output: [2 1 0 1]\n",
    "print(\"****************************\")\n",
    "\n",
    "'''\n",
    "2. One-Hot Encoding\n",
    "\n",
    "    Converts each category into a binary vector (0 or 1).\n",
    "\n",
    "    Good for nominal data (no order, e.g., color, city).\n",
    "'''\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "categories = np.array([['Red'], ['Green'], ['Blue'], ['Green']])\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded = encoder.fit_transform(categories)\n",
    "print(encoded)\n",
    "print(\"****************************\")\n",
    "\n",
    "\n",
    "\n",
    "#3. Binary / Custom Encoding\n",
    "\n",
    "   # Maps categories to binary representations or custom numeric codes.\n",
    "\n",
    "    \n",
    "    #useful when you want compact encoding for high-cardinality features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fd6cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
